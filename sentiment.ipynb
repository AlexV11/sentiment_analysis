{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras as ks\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = ks.datasets.imdb.load_data()\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = ks.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]], dtype=int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))\n",
    "# faaaaaantastic obtiene la posición 10053 porque no está en el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "782/782 [==============================] - 144s 173ms/step - loss: 0.6072 - accuracy: 0.6390\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 116s 148ms/step - loss: 0.3861 - accuracy: 0.8293\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 116s 149ms/step - loss: 0.2324 - accuracy: 0.9111\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 127s 162ms/step - loss: 0.1404 - accuracy: 0.9499\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.1112 - accuracy: 0.9610\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 137s 175ms/step - loss: 0.0892 - accuracy: 0.9691\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 136s 174ms/step - loss: 0.0644 - accuracy: 0.9773\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 136s 174ms/step - loss: 0.0491 - accuracy: 0.9836\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 136s 174ms/step - loss: 0.0468 - accuracy: 0.9840\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 138s 177ms/step - loss: 0.0391 - accuracy: 0.9873\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 138s 177ms/step - loss: 0.0315 - accuracy: 0.9895\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 139s 178ms/step - loss: 0.0326 - accuracy: 0.9897\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 138s 177ms/step - loss: 0.0272 - accuracy: 0.9914\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 138s 177ms/step - loss: 0.0255 - accuracy: 0.9916\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 137s 176ms/step - loss: 0.0270 - accuracy: 0.9909\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 138s 176ms/step - loss: 0.0212 - accuracy: 0.9925\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 138s 176ms/step - loss: 0.0200 - accuracy: 0.9930\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 139s 177ms/step - loss: 0.0190 - accuracy: 0.9940\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 139s 177ms/step - loss: 0.0152 - accuracy: 0.9954\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 138s 177ms/step - loss: 0.0149 - accuracy: 0.9955\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 138s 176ms/step - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 137s 176ms/step - loss: 0.0127 - accuracy: 0.9959\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 132s 169ms/step - loss: 0.0095 - accuracy: 0.9970\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.0101 - accuracy: 0.9968\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 132s 169ms/step - loss: 0.0053 - accuracy: 0.9985\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 130s 166ms/step - loss: 0.0040 - accuracy: 0.9988\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.0051 - accuracy: 0.9983\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.0065 - accuracy: 0.9980\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.0034 - accuracy: 0.9990\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 129s 164ms/step - loss: 0.0065 - accuracy: 0.9984\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 6.3442e-04 - accuracy: 0.9998\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.0043 - accuracy: 0.9986\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.0031 - accuracy: 0.9990\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.0024 - accuracy: 0.9992\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 9.9242e-04 - accuracy: 0.9998\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.0026 - accuracy: 0.9991\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 129s 164ms/step - loss: 0.0017 - accuracy: 0.9994\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.0025 - accuracy: 0.9993\n"
     ]
    }
   ],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess).map(encode_words).prefetch(1)\n",
    "\n",
    "embed_size = 128\n",
    "model = ks.models.Sequential([\n",
    "    ks.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]),\n",
    "    ks.layers.GRU(128, return_sequences=True),\n",
    "    ks.layers.GRU(128),\n",
    "    ks.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=100, callbacks=[ks.callbacks.EarlyStopping(monitor='accuracy', patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment100.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'sentiment100.pkl'\n",
    "joblib.dump(model, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the sentiment of a review\n",
    "def predict_sentiment(review, model):\n",
    "    encoded_review = table.lookup(tf.constant([review.split()]))\n",
    "    result = model.predict(tf.constant(encoded_review))[0][0]\n",
    "    if result < 0.5:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "    \n",
    "predict_sentiment(\"I love this movie\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
